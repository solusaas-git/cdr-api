# CDR API Batch Size Guide

## Performance Comparison

Based on real-world testing with your indexed database:

| Batch Size | Duration | Payload | Records/Sec | Batches for 10M | Total Time |
|------------|----------|---------|-------------|-----------------|------------|
| 10,000 | 0.9s | 9.4 MB | 11,111 | 1,000 | ~15 min |
| 25,000 | 1.7s | 23.5 MB | 14,700 | 400 | ~11 min |
| 50,000 | 3.3s | 47 MB | 15,150 | 200 | ~11 min |
| **100,000** âœ… | **4.8s** | **94 MB** | **20,800** | **100** | **~8 min** |

## Recommended Batch Sizes

### ðŸš€ Bulk Export / Data Migration
**Use: 100,000 records per batch**

```typescript
const BATCH_SIZE = 100000;

fetch(`/cdrs?i_account=${accountId}&limit=${BATCH_SIZE}&cursor=${cursor}`)
```

**Best for:**
- Exporting large datasets (millions of records)
- Data migration
- Batch processing
- Background jobs
- Maximum throughput

**Advantages:**
- âœ… Highest records/second (20,800)
- âœ… Fewest HTTP requests
- âœ… Lowest overhead
- âœ… Completes fastest

**Trade-offs:**
- âš ï¸ 4.8s per batch (still fast!)
- âš ï¸ 94 MB payload (fine for backend)

---

### ðŸ“Š Dashboard / Reports
**Use: 50,000 records per batch**

```typescript
const BATCH_SIZE = 50000;

fetch(`/cdrs?i_account=${accountId}&limit=${BATCH_SIZE}&cursor=${cursor}`)
```

**Best for:**
- Dashboard data loading
- Report generation
- Analytics queries
- Admin panels

**Advantages:**
- âœ… Fast response (3.3s)
- âœ… Good throughput
- âœ… Reasonable payload size
- âœ… Balance between speed and UX

**Trade-offs:**
- Takes 2x more requests than 100k batches

---

### ðŸ‘¤ User-Facing UI / Infinite Scroll
**Use: 25,000 records per batch**

```typescript
const BATCH_SIZE = 25000;

fetch(`/cdrs?i_account=${accountId}&limit=${BATCH_SIZE}&cursor=${cursor}`)
```

**Best for:**
- User-facing tables
- Infinite scroll
- Real-time displays
- Mobile apps

**Advantages:**
- âœ… Fastest initial response (1.7s)
- âœ… Lower memory per batch
- âœ… Smooth UX
- âœ… Better for slower connections

**Trade-offs:**
- Takes 4x more requests than 100k batches

---

### âš¡ Real-time / Live Updates
**Use: 10,000 records per batch**

```typescript
const BATCH_SIZE = 10000;

fetch(`/cdrs?i_account=${accountId}&limit=${BATCH_SIZE}&cursor=${cursor}`)
```

**Best for:**
- Live call monitoring
- Real-time updates
- Very slow connections
- Mobile with poor signal

**Advantages:**
- âœ… Sub-second response
- âœ… Minimal memory
- âœ… Instant feedback

**Trade-offs:**
- Lower overall throughput
- Many more HTTP requests

---

## Migration from XML-RPC

### Before (XML-RPC)
```python
# XML-RPC was slow, so you used small batches
batch_size = 25000  # Limited by XML-RPC overhead
```

### After (REST API with Indexes)
```typescript
// REST API is 10x faster, use larger batches!
const BATCH_SIZE = 100000;  // 4x larger, still fast!
```

**Why the change?**
- âœ… REST API has less overhead than XML-RPC
- âœ… Database indexes make queries 80% faster
- âœ… JSON parsing is faster than XML
- âœ… HTTP/2 multiplexing (if enabled)

---

## Example: Fetching 10M Records

### Old Way (25k batches)
```
400 batches Ã— 1.7s = 680 seconds = ~11 minutes
```

### New Way (100k batches)
```
100 batches Ã— 4.8s = 480 seconds = ~8 minutes
```

**Savings: 3 minutes (27% faster!)**

---

## Code Examples

### Progressive Loading with 100k Batches

```typescript
async function fetchAllCDRs(accountId: number, startDate: string, endDate: string) {
  const allRecords: CDR[] = [];
  let cursor: string | null = null;
  let batchNumber = 1;
  
  while (true) {
    const params = new URLSearchParams({
      i_account: accountId.toString(),
      limit: '100000',  // Large batches for speed
      start_date: startDate,
      end_date: endDate,
      type: 'non_zero_and_errors'
    });
    
    if (cursor) {
      params.append('cursor', cursor);
    }
    
    const response = await fetch(`/cdrs?${params}`);
    const result = await response.json();
    
    allRecords.push(...result.data);
    
    console.log(`Batch ${batchNumber}: ${result.data.length} records (total: ${allRecords.length})`);
    
    if (!result.next_cursor || result.data.length === 0) {
      break;
    }
    
    cursor = result.next_cursor;
    batchNumber++;
  }
  
  return allRecords;
}
```

### Parallel Fetching (First 5 Batches)

```typescript
async function fetchCDRsParallel(accountId: number, startDate: string, endDate: string) {
  // Fetch first 5 batches in parallel (500k records)
  const firstBatches = await Promise.all([
    fetchBatch(accountId, 0, 100000),
    fetchBatch(accountId, 100000, 100000),
    fetchBatch(accountId, 200000, 100000),
    fetchBatch(accountId, 300000, 100000),
    fetchBatch(accountId, 400000, 100000),
  ]);
  
  const allRecords = firstBatches.flatMap(b => b.data);
  
  // Continue with cursor from last batch
  let cursor = firstBatches[4].next_cursor;
  
  while (cursor) {
    const result = await fetchWithCursor(accountId, cursor, 100000);
    allRecords.push(...result.data);
    cursor = result.next_cursor;
  }
  
  return allRecords;
}
```

---

## Memory Considerations

### Browser Memory Limits

| Batch Size | Memory per Batch | 10M Records Total |
|------------|------------------|-------------------|
| 25,000 | ~24 MB | ~960 MB |
| 50,000 | ~47 MB | ~940 MB |
| 100,000 | ~94 MB | ~940 MB |

**Note:** Total memory is similar because you're loading the same data. Larger batches just mean fewer trips.

### Backend Memory

For backend processing (Node.js), 100k batches are fine:
- Node.js can easily handle 94 MB payloads
- Streaming JSON parsing available if needed
- Can process and discard batches to keep memory low

---

## Network Considerations

### Bandwidth Usage

All batch sizes use the same total bandwidth (you're fetching the same data).

**Difference is in overhead:**
- 100k batches: 100 HTTP requests
- 25k batches: 400 HTTP requests

**HTTP overhead saved with 100k batches:**
- ~300 fewer HTTP handshakes
- ~300 fewer TLS negotiations
- Less connection overhead

---

## Recommendations Summary

| Use Case | Batch Size | Why |
|----------|------------|-----|
| Bulk Export | 100,000 | Maximum speed |
| Reports/Analytics | 50,000 | Balance |
| User Tables | 25,000 | Fast response |
| Real-time | 10,000 | Instant feedback |

**Default recommendation: Start with 100,000 for backend, 50,000 for frontend.**

You can always adjust based on your specific needs!

